{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# import pandas as pd\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tag import pos_tag\n",
    "import networkx as nx\n",
    "# from numpy.random import rand, RandomState\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import KMeans\n",
    "import string\n",
    "# from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.tokenize import word_tokenize\n",
    "# import matplotlib\n",
    "# from mpl_toolkits.basemap import Basemap\n",
    "# import geopy\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "# from geopy.geocoders import Nominatim\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import itertools\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def json_to_dict(filename):\n",
    "    '''\n",
    "    INPUT: name of json file\n",
    "    OUTPUT: dictionary with city keys and description values\n",
    "    take json file and return dictionary\n",
    "    '''\n",
    "    di = {}\n",
    "    english = []\n",
    "    with open('wordlist.txt') as f:\n",
    "        for line in f:\n",
    "            english.append(line.strip('\\r\\n'))\n",
    "            english.append(' ')\n",
    "    english = set(english)\n",
    "\n",
    "    with open(filename) as data_file:\n",
    "        data = json.load(data_file)\n",
    "        for item in data:\n",
    "            for key in item.keys():\n",
    "                # remove punctuation, make everything lower case\n",
    "                txt = ''.join(ch.lower() for ch in item[key] if ch not in set(string.punctuation))\n",
    "                # remove numbers\n",
    "                txt = ''.join(c for c in txt if c.isdigit() is False)\n",
    "                # remove the city and country name from its own description\n",
    "                keyparts = key.split(',')\n",
    "                txt = ' '.join(c for c in txt.split() if c.lower() != keyparts[0].lower() and c.lower() != keyparts[-1].lower())\n",
    "                # remove non-english words\n",
    "                txt = ' '.join(c for c in txt.split() if c in english)\n",
    "                # populate dictionary\n",
    "                di[key.strip().strip('\\n').encode('ascii', 'ignore')] = txt\n",
    "    return di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_synonym_list(word_list):\n",
    "    '''\n",
    "    INPUT: list of words\n",
    "    OUTPUT: list containing words and all of their synonyms\n",
    "    take a list of words and return a list containin all synonyms of those words\n",
    "    '''\n",
    "    all_synonyms = []\n",
    "    for word in word_list:\n",
    "        synonyms = wn.synsets(word.lower())\n",
    "        for s in synonyms:\n",
    "            synlist = [l.name() for l in s.lemmas()]\n",
    "        synlist.append(word.lower())\n",
    "        all_synonyms.extend([wn.lemmatize(w) for w in synlist])\n",
    "    return all_synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the data and prepare for tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rs_dict = json_to_dict('../data/ricksteves_articles_blogs_R01.json')\n",
    "ta_dict = json_to_dict('../data/europe_city_reviews2.json')\n",
    "\n",
    "empty_count = 0\n",
    "for k in ta_dict:\n",
    "    if len(ta_dict[k]) < 100:\n",
    "        empty_count += 1\n",
    "# print empty_count\n",
    "\n",
    "key_list = set(rs_dict.keys() + ta_dict.keys())\n",
    "europe_dict = dict()\n",
    "for key in key_list:\n",
    "    europe_dict[key] = str(rs_dict.get(key)) + str(ta_dict.get(key))\n",
    "    \n",
    "# remove cities which contain little or no text\n",
    "europe_dict = {key: value for key, value in europe_dict.items() if len(value) > 200}\n",
    "for k in europe_dict:\n",
    "    if len(europe_dict[k]) < 100:\n",
    "        print '\\n', k\n",
    "        print europe_dict[k]\n",
    "\n",
    "# Convert dictionary into dataframe\n",
    "cities_df = pd.DataFrame.from_dict(europe_dict, orient='index', dtype=None)\n",
    "cities_df.columns = ['description']\n",
    "\n",
    "# Extract cities and their descripitions from dataframe\n",
    "# doc_bodies = cities_df['description']\n",
    "doc_bodies = cities_df['description'].values\n",
    "tokenized_corpus = [word_tokenize(content.lower()) for content in doc_bodies]\n",
    "cities = cities_df.index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total count and document frequency for nouns, verbs, adverbs, and adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read each word, assign pos_tag. If pos tag is noun, append dictionary\n",
    "noun_dict = defaultdict(list)\n",
    "verb_dict = defaultdict(list)\n",
    "adj_dict = defaultdict(list)\n",
    "adv_dict = defaultdict(list)\n",
    "other_dict = defaultdict(list)\n",
    "\n",
    "for i, doc in enumerate(tokenized_corpus):\n",
    "    for word in doc:\n",
    "        tag = pos_tag([word])[0][1]\n",
    "        if 'NN' in tag:\n",
    "            noun_dict[word].append(i)\n",
    "        if 'JJ' in tag:\n",
    "            adj_dict[word].append(i)\n",
    "        if 'VB' in tag:\n",
    "            verb_dict[word].append(i)\n",
    "        if 'RB' in tag:\n",
    "            adv_dict[word].append(i)\n",
    "        if tag == 'EX' or tag == 'IN' or tag == 'FW' or tag == 'PDT' or tag == 'CD' or tag == 'MD':\n",
    "            other_dict[word].append(i)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def kmeans_clustering(doc_bodies, n=12, stpwords=[]):\n",
    "    vectorizer = TfidfVectorizer(stop_words=stopwords.words('english') + stpwords)\n",
    "    X = vectorizer.fit_transform(doc_bodies)\n",
    "    features = vectorizer.get_feature_names()\n",
    "    kmeans = KMeans(n_clusters=n)\n",
    "    kmeans.fit(X)\n",
    "\n",
    "    top_centroids = kmeans.cluster_centers_.argsort()[:, -1:-8:-1]\n",
    "    print \"top features for each cluster:\"\n",
    "    for num, centroid in enumerate(top_centroids):\n",
    "        print \"%d: %s\" % (num, \", \".join(features[i] for i in centroid))\n",
    "    return kmeans, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_cluster_center(word_list):\n",
    "    wordnet = WordNetLemmatizer()\n",
    "    word_dict = dict()  # record synonyms and lemmas for each word in a dictionary\n",
    "    for word in word_list:\n",
    "        subset = [word]\n",
    "        for s in wn.synsets(word.lower()):\n",
    "            synlist = [l.name() for l in s.lemmas()]\n",
    "            subset.extend([wordnet.lemmatize(w) for w in synlist])\n",
    "        word_dict[word] = set(subset)\n",
    "    syns = set([item for sublist in word_dict.values() for item in sublist])\n",
    "\n",
    "    edge_list = []\n",
    "    for syn in syns: \n",
    "        keys_lst = [key for key, value in word_dict.items() if syn in value]\n",
    "        if len(keys_lst) > 1:\n",
    "            edge_list.extend(list(itertools.combinations(keys_lst, 2)))\n",
    "    G=nx.Graph()\n",
    "    G.add_edges_from(edge_list)\n",
    "\n",
    "    max_deg = Counter(nx.degree_centrality(G)).most_common(1)[0][0]\n",
    "    nbrs = G.neighbors(max_deg)\n",
    "    return max_deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_stats(noun_dict, verb_dict, adj_dict, adv_dict, other_dict, word):\n",
    "    parts_of_speech = ['noun', 'verb', 'adjective', 'adverb', 'other']\n",
    "    \n",
    "    noun_stats = noun_dict.get(word)\n",
    "    verb_stats = verb_dict.get(word)\n",
    "    adj_stats = adj_dict.get(word)\n",
    "    adv_stats = adv_dict.get(word)\n",
    "    other_stats = other_dict.get(word)\n",
    "    \n",
    "    speech_parts = np.array([noun_stats, verb_stats, adj_stats, adv_stats, other_stats])\n",
    "    \n",
    "    word_pos = np.argmax(speech_parts)\n",
    "    \n",
    "    return parts_of_speech[word_pos], word_pos, len(speech_parts[word_pos]), len(set(speech_parts[word_pos]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_stopwords_count(speech_dicts, word_count, word_pos, greater_than=True):\n",
    "    vals = speech_dicts[word_pos].values()\n",
    "    wds = np.array(speech_dicts[word_pos].keys())\n",
    "    word_counts = np.array([len(val) for val in vals])\n",
    "    if greater_than:\n",
    "        return wds[word_counts >= word_count]\n",
    "    else:\n",
    "        return wds[word_counts <= word_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_stopwords(speech_dicts, doc_frequency, word_pos, greater_than=True):\n",
    "    vals = speech_dicts[word_pos].values()\n",
    "    wds = np.array(speech_dicts[word_pos].keys())\n",
    "    doc_freqs = np.array([len(set(val)) for val in vals])\n",
    "    if greater_than:\n",
    "        return wds[doc_freqs >= doc_frequency]\n",
    "    else:\n",
    "        return wds[doc_freqs <= doc_frequency]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_cosine_sim(doc_bodies, check_words, stops):\n",
    "    vectorizer = TfidfVectorizer(stop_words=stopwords.words('english') + stops)\n",
    "\n",
    "    X = vectorizer.fit_transform(doc_bodies)\n",
    "    c = vectorizer.transform(check_words)\n",
    "    print X.shape\n",
    "    print c.shape\n",
    "    # print len(doc_bodies)\n",
    "    # 1. Compute cosine similarity\n",
    "    cosine_similarities = linear_kernel(X, X)\n",
    "\n",
    "    sims = np.zeros([len(db),])\n",
    "    # 2. Print out similarities\n",
    "    for i, doc1 in enumerate(db):\n",
    "        for j, doc2 in enumerate(db):\n",
    "            if i == len(db) - 1 and j < len(db) -1:\n",
    "                sims[j] = cosine_similarities[i, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top features for each cluster:\n",
      "0: introduction, affiliated, swim, head, valuable, water, offered\n",
      "1: museum, irish, city, dutch, french, andorra, port\n",
      "2: cave, wildlife, see, speciality, bird, visitor, well\n",
      "3: nice, place, beautiful, see, great, one, worth\n",
      "4: great, good, place, well, lovely, staff, visit\n",
      "5: organic, smell, products, booking, professional, range, clean\n",
      "6: tower, leaning, baptistery, field, clamber, bell, artistically\n",
      "7: mosque, dean, dervish, meaning, us, trance, alluring\n",
      "8: bulgaria, communist, slavic, bulgarian, disclosing, directional, alphabet\n",
      "9: unpolished, candid, lousy, dimensions, gritty, impromptu, europe\n",
      "10: walk, cinder, path, along, flat, estuary, views\n",
      "11: city, one, rick, like, great, people, tour\n",
      "stars noun 41\n",
      "go verb 143\n",
      "take verb 143\n",
      "get verb 145\n",
      "get verb 145\n",
      "feel noun 121\n",
      "make verb 143\n",
      "filmed verb 20\n",
      "learned verb 72\n",
      "looks noun 112\n",
      "track noun 44\n",
      "get verb 145\n"
     ]
    }
   ],
   "source": [
    "# create clusters\n",
    "kmeans, features = kmeans_clustering(doc_bodies, n=12, stpwords=[])\n",
    "# kmeans = kmeans_clustering(X,)\n",
    "top_centroids = kmeans.cluster_centers_.argsort()[:, -1:-51:-1]\n",
    "cluster_cents = []\n",
    "stp_wds = []\n",
    "for i, centroid in enumerate(top_centroids):\n",
    "    cent = get_cluster_center([features[j] for j in centroid])\n",
    "    pos, word_pos, overallcount, doc_freq = get_word_stats(noun_dict, verb_dict, adj_dict, adv_dict, other_dict, cent)\n",
    "    print cent, pos, doc_freq\n",
    "    if doc_freq > 100:\n",
    "        stp_wds.extend(get_stopwords(np.array([noun_dict, verb_dict, adj_dict, adv_dict, other_dict]), doc_freq, word_pos))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l = sorted(get_stopwords(np.array([noun_dict, verb_dict, adj_dict, adv_dict, other_dict]), 118, 1, greater_than=True))\n",
    "m = sorted(get_stopwords_count(np.array([noun_dict, verb_dict, adj_dict, adv_dict, other_dict]), 1385, 0, greater_than=True))\n",
    "mm = sorted(get_stopwords_count(np.array([noun_dict, verb_dict, adj_dict, adv_dict, other_dict]), 5, 1, greater_than=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stp_wds = []\n",
    "# nouns\n",
    "stp_wds.extend(get_stopwords(np.array([noun_dict, verb_dict, adj_dict, adv_dict, other_dict]), 85, 0, greater_than=True))\n",
    "stp_wds.extend(get_stopwords(np.array([noun_dict, verb_dict, adj_dict, adv_dict, other_dict]), 1, 0, greater_than=False))\n",
    "stp_wds.extend(get_stopwords_count(np.array([noun_dict, verb_dict, adj_dict, adv_dict, other_dict]), 1385, 0, greater_than=True))\n",
    "stp_wds.extend(get_stopwords(np.array([noun_dict, verb_dict, adj_dict, adv_dict, other_dict]), 0, 0, greater_than=True))\n",
    "\n",
    "\n",
    "\n",
    "# verbs\n",
    "stp_wds.extend(get_stopwords(np.array([noun_dict, verb_dict, adj_dict, adv_dict, other_dict]), 86, 1, greater_than=True))\n",
    "stp_wds.extend(get_stopwords_count(np.array([noun_dict, verb_dict, adj_dict, adv_dict, other_dict]), 165, 1, greater_than=True))\n",
    "stp_wds.extend(get_stopwords_count(np.array([noun_dict, verb_dict, adj_dict, adv_dict, other_dict]), 2, 1, greater_than=False))\n",
    "# adjectives\n",
    "stp_wds.extend(get_stopwords(np.array([noun_dict, verb_dict, adj_dict, adv_dict, other_dict]), 78, 2, greater_than=True))\n",
    "stp_wds.extend(get_stopwords_count(np.array([noun_dict, verb_dict, adj_dict, adv_dict, other_dict]), 925, 2, greater_than=True))\n",
    "stp_wds.extend(get_stopwords_count(np.array([noun_dict, verb_dict, adj_dict, adv_dict, other_dict]), 1, 2, greater_than=False))\n",
    "# adverbs\n",
    "stp_wds.extend(get_stopwords(np.array([noun_dict, verb_dict, adj_dict, adv_dict, other_dict]), 118, 3, greater_than=True))\n",
    "stp_wds.extend(get_stopwords_count(np.array([noun_dict, verb_dict, adj_dict, adv_dict, other_dict]), 2645, 3, greater_than=True))\n",
    "# other\n",
    "stp_wds.extend(get_stopwords(np.array([noun_dict, verb_dict, adj_dict, adv_dict, other_dict]), 0, 4, greater_than=True))\n",
    "# Two letter words\n",
    "for lst in tokenized_corpus:\n",
    "    stp_wds.extend([word for word in lst if len(word) < 3])\n",
    "# nationality words\n",
    "stp_wds.extend('''bulgarian,german,germany,portuguese,italy,italian,spain,alcazar,turkey,turkish,polish,poland,scottish,\n",
    "               bulgaria,french,france,rick,iceland,croatia,spanish,irish,london,norwegian,danish,andersen,swedish,\n",
    "               elbe,liechtenstein,rome,albanian,swiss,dutch,netherlands,ireland,estonian,latvian,slovenian,soviet,estonia,\n",
    "               bulgaria,norway,czech,edinburgh,belgian,belgium,romania,bulgaria,finnish'''.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8854\n",
      "2144\n",
      "222\n",
      "False\n",
      "True\n",
      "['absorb', 'account', 'acts', 'adventures', 'ale', 'amateur', 'archaeology', 'assistance', 'assistant', 'attempt', 'attendant', 'austria', 'beats', 'bishop', 'bizarre', 'blast', 'bold', 'brave', 'brown', 'bursting', 'campus', 'candy', 'capacity', 'capped', 'carriage', 'casa', 'cater', 'cherry', 'circus', 'classes', 'coastal', 'coats', 'column', 'combo', 'complaint', 'contain', 'convent', 'cooler', 'costume', 'creepy', 'crystal', 'dame', 'daniel', 'daylight', 'decorate', 'deeply', 'des', 'dine', 'dinosaur', 'dirt', 'don', 'du', 'duke', 'efforts', 'element', 'encounter', 'engine', 'enjoyment', 'establishment', 'eve', 'exact', 'fake', 'fitting', 'flag', 'flea', 'floodlit', 'flora', 'fluent', 'fond', 'forth', 'fortune', 'foyer', 'frank', 'function', 'funicular', 'gathering', 'gear', 'gourmet', 'grandiose', 'graves', 'greet', 'grocery', 'guidance', 'halloween', 'handmade', 'handsome', 'hassle', 'hate', 'headquarters', 'hearty', 'hillside', 'hilltop', 'hint', 'hopes', 'hospitality', 'hubby', 'hunt', 'hurry', 'icon', 'id', 'ignore', 'impressionist', 'improvement', 'inn', 'installation', 'intense', 'interaction', 'introduce', 'joyful', 'jumping', 'knack', 'l', 'las', 'legs', 'lesser', 'letters', 'linger', 'lion', 'load', 'lobby', 'locate', 'manor', 'massage', 'memorials', 'mess', 'message', 'midst', 'motor', 'mouth', 'mozart', 'mr', 'mud', 'muslim', 'notre', 'opulent', 'organization', 'orientation', 'pain', 'passage', 'passport', 'pastry', 'pavement', 'personality', 'personnel', 'persons', 'photographer', 'pine', 'pink', 'pipe', 'plane', 'plaque', 'polished', 'politics', 'praise', 'pray', 'premises', 'press', 'pristine', 'prize', 'programme', 'protective', 'pulse', 'raise', 'recall', 'record', 'reference', 'relaxation', 'repair', 'reputation', 'resist', 'restore', 'revisit', 'roots', 'rope', 'sanctuary', 'scooter', 'script', 'seafood', 'seaside', 'sisters', 'sleek', 'slick', 'smoke', 'smoking', 'snap', 'soup', 'source', 'spa', 'stages', 'strip', 'subject', 'success', 'sufficient', 'survey', 'sweets', 'symphony', 'tale', 'task', 'tennis', 'testament', 'tide', 'tile', 'torn', 'translate', 'treatment', 'union', 'update', 'vary', 'victoria', 'voucher', 'waiter', 'wave', 'weapons', 'web', 'weight', 'win', 'winds', 'wise', 'worlds', 'writer', 'yo', 'youthful']\n"
     ]
    }
   ],
   "source": [
    "upper = 10\n",
    "lower = 1\n",
    "up = get_stopwords(np.array([noun_dict, verb_dict, adj_dict, adv_dict, other_dict]), 14, 0, greater_than=False)\n",
    "low = get_stopwords(np.array([noun_dict, verb_dict, adj_dict, adv_dict, other_dict]), 13, 0, greater_than=True)\n",
    "print len(up)\n",
    "print len(low)\n",
    "print len(set(sorted(up)) & set(sorted(low)))\n",
    "m = sorted(list(set(up) & set(low)))\n",
    "print 'cheerful' in m\n",
    "print 'joyful' in m\n",
    "\n",
    "print m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try snowball and porter stemmers prior to tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snowball = SnowballStemmer('english')\n",
    "docs_snowball = [' '.join([snowball.stem(word) for word in words.split()]) for words in doc_bodies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dict_snowball = defaultdict(list)\n",
    "# for each word, check if adjective. if adjective, add original word to dictionary list\n",
    "for words in doc_bodies:\n",
    "    for word in words.split():\n",
    "        if 'JJ' in pos_tag([word])[0][1]:\n",
    "            dict_snowball[snowball.stem(word)].append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9853\n"
     ]
    }
   ],
   "source": [
    "stopwds = []\n",
    "for words in docs_snowball:\n",
    "    for word in words.split():\n",
    "        if len(dict_snowball[word]) < 1:\n",
    "            stopwds.append(word)\n",
    "\n",
    "            pos_dicts = np.array([noun_dict, verb_dict, adj_dict, adv_dict, other_dict])           \n",
    "stopwds.extend([snowball.stem(word) for word in get_stopwords(pos_dicts, 78, 2, greater_than=True)])\n",
    "stopwds.extend([snowball.stem(word) for word in get_stopwords_count(pos_dicts, 925, 2, greater_than=True)])\n",
    "stopwds.extend([snowball.stem(word) for word in get_stopwords_count(pos_dicts, 1, 2, greater_than=False)])\n",
    "\n",
    "removelist = '''swedish,french,polish,swiss,italian,likely,finnish,german,norwegian,portuguese,american,spanish,\\\n",
    "scottish,russian,belgian,irish,actual,tuscan,arrive,vatican,affected,especial,british,nordic,flemish,venetian,\\\n",
    "soviet,golden,unknown,possible,detailed,useful,vertical,alphabetical,noteworthy,basic,valuable,slovenian,manageable,\\\n",
    "experienced,particular,numbered,essential,final,estuary,weekly,regular,flat,related,continuous,continued,mixed,\\\n",
    "estonian,advisable,characteristic,satisfied,prepared,baltic,similar,georgian'''.split(',')\n",
    "stopwds.extend([snowball.stem(word) for word in removelist])\n",
    "print len(set(stopwds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top features for each cluster:\n",
      "0: inform, art, option, natur, incred, person, music\n",
      "1: art, architectur, inform, access, attract, certain, probabl\n",
      "2: inform, thought, attract, atmospher, access, person, art\n",
      "3: art, cultur, rich, civil, black, pedestrian, music\n",
      "4: urban, oblivi, general, cost, contagi, inflat, competit\n",
      "5: sculptur, photograph, art, attent, educ, funerari, inform\n",
      "6: cost, bigger, marvel, tini, current, classic, thought\n",
      "7: imagin, safe, inspir, strong, thought, music, delight\n",
      "8: nervous, pleas, apprehens, fabul, etern, oldest, adventur\n",
      "9: mysteri, mystic, ritual, conserv, pompous, ey, overdu\n",
      "10: profession, product, organ, zoolog, floral, fortuit, formid\n",
      "11: art, cultur, unpolish, nake, dramat, rich, sculptur\n",
      "\n",
      "\n",
      "informal, informative, artful, optional, natural, incredible, personal, personable, musical\n",
      "artful, architectural, informal, informative, accessible, attractive, certain, probable\n",
      "informal, informative, thoughtful, attractive, atmospheric, accessible, personal, personable, artful\n",
      "artful, cultural, rich, civil, black, pedestrian, musical\n",
      "urban, oblivious, general, costly, contagious, inflatable, competitive\n",
      "sculptural, photographic, artful, attentive, educational, educative, funerary, informal, informative\n",
      "costly, bigger, marvelous, tiny, current, classical, classic, thoughtful\n",
      "imaginative, safe, inspirational, strong, thoughtful, musical, delighted\n",
      "nervous, pleased, apprehensive, fabulous, eternal, oldest, adventurous\n",
      "mysterious, mystical, mystic, ritual, conservative, pompous, overdue\n",
      "professional, productive, organic, zoological, floral, fortuitous, formidable\n",
      "artful, cultural, unpolished, naked, dramatic, rich, sculptural\n"
     ]
    }
   ],
   "source": [
    "kmeans, features = kmeans_clustering(docs_snowball, n=12, stpwords=list(set(stopwds)))\n",
    "# kmeans = kmeans_clustering(X,)\n",
    "top_centroids = kmeans.cluster_centers_.argsort()[:, -1:-8:-1]\n",
    "cluster_cents = []\n",
    "print '\\n'\n",
    "for i, centroid in enumerate(top_centroids):\n",
    "    wordlist = []\n",
    "\n",
    "    for j in centroid:\n",
    "        wordlist.extend(list(set(dict_snowball[features[j]])))\n",
    "    print ', '.join(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['peaceful', 'calm', 'tranquil', 'serene', 'relax', 'quiet', 'harmonious', 'relaxed', 'easygoing', 'serenity', 'relaxation', 'carefree', 'comfortable', 'sedate', 'pastoral', 'decompress', 'laid_back', 'comfortable', 'easy', 'calming', 'relaxing', 'comforting', 'unwind', 'rest', 'repose', 'low_key', 'soothing', 'restful']\n",
      "(169, 8617)\n",
      "(28, 8617)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'db' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-eff6c68a70eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mpersonality\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m', '\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# read words on each line of file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mpersonality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mcalculate_cosine_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_bodies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpersonality\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-102-4f42b2193abf>\u001b[0m in \u001b[0;36mcalculate_cosine_sim\u001b[0;34m(doc_bodies, check_words, stops)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mcosine_similarities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0msims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;31m# 2. Print out similarities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'db' is not defined"
     ]
    }
   ],
   "source": [
    "with open('personalities.txt') as f:\n",
    "    for line in f:\n",
    "        personality = line.strip('\\n').split(', ') # read words on each line of file\n",
    "        print personality\n",
    "        calculate_cosine_sim(doc_bodies, personality, stopwds)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
