{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tag import pos_tag\n",
    "import networkx as nx\n",
    "from numpy.random import rand, RandomState\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import KMeans\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import geopy\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from geopy.geocoders import Nominatim\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def json_to_dict(filename):\n",
    "    '''\n",
    "    INPUT: name of json file\n",
    "    OUTPUT: dictionary with city keys and description values\n",
    "    take json file and return dictionary\n",
    "    '''\n",
    "    di = {}\n",
    "    english = []\n",
    "    with open('wordlist.txt') as f:\n",
    "        for line in f:\n",
    "            english.append(line.strip('\\r\\n'))\n",
    "            english.append(' ')\n",
    "    english = set(english)\n",
    "\n",
    "    with open(filename) as data_file:\n",
    "        data = json.load(data_file)\n",
    "        for item in data:\n",
    "            for key in item.keys():\n",
    "                # remove punctuation, make everything lower case\n",
    "                txt = ''.join(ch.lower() for ch in item[key] if ch not in set(string.punctuation))\n",
    "                # remove numbers\n",
    "                txt = ''.join(c for c in txt if c.isdigit() is False)\n",
    "                # remove the city name from its own description\n",
    "                keyparts = key.split(',')\n",
    "                txt = ' '.join(c for c in txt.split() if c != keyparts[0].lower() and c != keyparts[-1].lower())\n",
    "                # remove non-english words\n",
    "                txt = ' '.join(c for c in txt.split() if c in english)\n",
    "                # populate dictionary\n",
    "                di[key.strip().strip('\\n').encode('ascii', 'ignore')] = txt\n",
    "    return di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_dictionaries(dict1, dict2, keys_file):\n",
    "    '''\n",
    "    INPUT: dictionaries and keys\n",
    "    OUTPUT: dictionary with combined data and final key names\n",
    "    take two dictionaries and combine into one\n",
    "    '''\n",
    "    combined_dict = {}\n",
    "    with open(keys_file) as f:\n",
    "        for line in f:\n",
    "            d1_key = line.split('|')[0]\n",
    "            d2_key = line.split('|')[1]\n",
    "            final_key = line.split('|')[2].strip('\\n')\n",
    "\n",
    "            if d2_key == 'noname':\n",
    "                combined_dict[final_key] = dict1.get(d1_key)\n",
    "            elif d1_key == 'noname':\n",
    "                combined_dict[final_key] = dict2.get(d2_key)\n",
    "            else:\n",
    "                combined_dict[final_key] = dict1.get(d1_key) + dict2.get(d2_key)\n",
    "    return combined_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(corpus, words_to_add=[]):\n",
    "    '''\n",
    "    INPUT: tokenized document corpus, list of words to remove\n",
    "    OUTPUT: document corpus with stop words removed\n",
    "    take a list of words to add to stoplist and remove from corpus\n",
    "    '''\n",
    "    '''Use the English language stopwords provided by nltk'''\n",
    "    stops = stopwords.words('english')\n",
    "    \n",
    "    '''Add user-requested stopwords'''\n",
    "    stops.extend(words_to_add)\n",
    "    \n",
    "    '''Remove stop words from corpus and return'''\n",
    "    #tokenized = [word_tokenize(content.lower()) for content in corpus]\n",
    "    stops = set(stops)\n",
    "    docs = [[word for word in words if word not in stops]\n",
    "        for words in corpus]\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_word_frequencies(tokenized_corpus, title):\n",
    "    '''\n",
    "    INPUT: tokenized corpus, plot title\n",
    "    OUTPUT: vocabulary list, word ids, word counts\n",
    "    take a tokenized corpus, calculate the frequency of each word, and plot\n",
    "    '''\n",
    "    '''Build a vocabulary of every unique word in the corpus'''\n",
    "    vocab_set = set()\n",
    "    [[vocab_set.add(token) for token in tokens] for tokens in tokenized_corpus]\n",
    "    vocab = list(vocab_set)\n",
    "    \n",
    "    '''Make a dictionary relating word to id number'''\n",
    "    vocab_dict = {word: i for i, word in enumerate(vocab)}\n",
    "    \n",
    "    '''Build word count matrix'''\n",
    "    word_ids = np.zeros(len(vocab))\n",
    "    word_counts = np.zeros((len(tokenized_corpus), len(vocab)))\n",
    "    for doc_id, words in enumerate(tokenized_corpus):\n",
    "        for word in words:\n",
    "            word_id = vocab_dict[word]\n",
    "            word_ids[word_id] = word_id\n",
    "            word_counts[doc_id][word_id] += 1\n",
    "    \n",
    "    '''Count the total number of times each word occurs'''\n",
    "    values = np.sum(word_counts, axis=0)\n",
    "    \n",
    "    '''Plot the results'''\n",
    "    plt.bar(range(0,len(values)), sorted(values)[::-1], color='green', alpha=0.4)\n",
    "\n",
    "    plt.xlabel('Word Index')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Word Frequency Chart - {}'.format(title))\n",
    "    return np.array(values), vocab, word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_synonym_list(word_list):\n",
    "    '''\n",
    "    INPUT: list of words\n",
    "    OUTPUT: list containing words and all of their synonyms\n",
    "    take a list of words and return a list containin all synonyms of those words\n",
    "    '''\n",
    "    all_synonyms = []\n",
    "    for word in word_list:\n",
    "        synonyms = wn.synsets(word.lower())\n",
    "        for s in synonyms:\n",
    "            synlist = [l.name() for l in s.lemmas()]\n",
    "        synlist.append(word.lower())\n",
    "        all_synonyms.extend([wordnet.lemmatize(w) for w in synlist])\n",
    "    return all_synonyms\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def limit_speech_part(corpus, pts_of_speech = ['ADJ', 'NOUN', 'ADV', 'VERB']):\n",
    "    '''\n",
    "    INPUT: tokenized corpus, list of parts of speech to keep\n",
    "    OUTPUT: list of words to remove\n",
    "    take a tokenized corpus and return a list of words to remove from that are not in the part of speech list\n",
    "    '''\n",
    "    remove_words = []\n",
    "    for words in corpus:\n",
    "        for word in words:\n",
    "            check = 0\n",
    "            for ps in pts_of_speech:\n",
    "                check += len(wn.synsets(word, pos=getattr(wn, ps)))\n",
    "            if check == 0:\n",
    "                remove_words.append(word)\n",
    "    return remove_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_personality_words():\n",
    "    with open('personalitywords.txt') as f:\n",
    "        personality_words = []\n",
    "        for line in f:\n",
    "            for word in line.split():\n",
    "                synonyms = wn.synsets(word.lower())\n",
    "                synlist = [word.lower()]\n",
    "                for s in synonyms:\n",
    "                    synlist = [l.name() for l in s.lemmas()]\n",
    "                    personality_words.extend([wordnet.lemmatize(w) for w in synlist if len(wn.synsets(w, pos=wn.ADJ))>0])\n",
    "    return set(personality_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_features_freq(city_list, cities, stop_wds, doc_bodies):\n",
    "    vectorizer = CountVectorizer(stop_words=stop_wds)\n",
    "    X = vectorizer.fit_transform(doc_bodies)\n",
    "    features = vectorizer.get_feature_names()\n",
    "    \n",
    "    x_dense = X.todense() # convert feature matrix to dense\n",
    "    for city in city_list:\n",
    "        ind = np.where(cities==city)[0][0]\n",
    "        city_words = np.array(x_dense[ind])[0]\n",
    "        top_indices = np.array(np.argsort(city_words)[::-1][:8])\n",
    "        print '{}: {}'.format(city, ', '.join(np.array(features)[top_indices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features_tfidf(city_list, cities, stop_wds, doc_bodies):\n",
    "    vectorizer = TfidfVectorizer(stop_words=stop_wds)\n",
    "    X = vectorizer.fit_transform(doc_bodies)\n",
    "    features = vectorizer.get_feature_names()\n",
    "    \n",
    "    x_dense = X.todense() # convert feature matrix to dense\n",
    "    for city in city_list:\n",
    "        ind = np.where(cities==city)[0][0]\n",
    "        city_words = np.array(x_dense[ind])[0]\n",
    "        top_indices = np.array(np.argsort(city_words)[::-1][:8])\n",
    "        print '{}: {}'.format(city, ', '.join(np.array(features)[top_indices]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial data formatting:\n",
    "Convert to dictionary, combine dictionaries, remove cities without text, convert to data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    }
   ],
   "source": [
    "rs_dict = json_to_dict('../scraping/ricksteves_articles_blogs_R01.json')\n",
    "ta_dict = json_to_dict('../scraping/europe_city_reviews2.json')\n",
    "\n",
    "empty_count = 0\n",
    "for k in ta_dict:\n",
    "    if len(ta_dict[k]) < 100:\n",
    "        empty_count += 1\n",
    "print empty_count\n",
    "\n",
    "key_list = set(rs_dict.keys() + ta_dict.keys())\n",
    "europe_dict = dict()\n",
    "for key in key_list:\n",
    "    europe_dict[key] = str(rs_dict.get(key)) + str(ta_dict.get(key))\n",
    "    \n",
    "# remove cities which contain little or no text\n",
    "europe_dict = {key: value for key, value in europe_dict.items() if len(value) > 200}\n",
    "for k in europe_dict:\n",
    "    if len(europe_dict[k]) < 100:\n",
    "        print '\\n', k\n",
    "        print europe_dict[k]\n",
    "\n",
    "# Convert dictionary into dataframe\n",
    "cities_df = pd.DataFrame.from_dict(europe_dict, orient='index', dtype=None)\n",
    "cities_df.columns = ['description']\n",
    "\n",
    "# Extract cities and their descripitions from dataframe\n",
    "doc_bodies = cities_df['description'].values\n",
    "tokenized_corpus = [word_tokenize(content.lower()) for content in doc_bodies]\n",
    "cities = cities_df.index.values\n",
    "\n",
    "wordnet = WordNetLemmatizer()\n",
    "docs_wordnet = [' '.join([wordnet.lemmatize(word) for word in words.split()]) for words in doc_bodies]\n",
    "tokenized_corpus_wn = [word_tokenize(content.lower()) for content in docs_wordnet]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk stop words removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Common Words:\n",
      "Venice, Italy: one, see, tour, worth, great, church, place, beautiful\n",
      "\n",
      "Tfidf Most Important Words:\n",
      "Venice, Italy: canal, venetian, gondola, tour, see, one, worth, church\n"
     ]
    }
   ],
   "source": [
    "tokenized_corpus_0 = remove_stopwords(tokenized_corpus)\n",
    "print 'Most Common Words:'\n",
    "get_features_freq(['Venice, Italy'], cities, stopwords.words('english'), doc_bodies)\n",
    "print '\\nTfidf Most Important Words:'\n",
    "get_features_tfidf(['Venice, Italy'], cities, stopwords.words('english'), doc_bodies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-personality words removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11425 stopwords removed\n",
      "\n",
      "Most Common Words:\n",
      "Venice, Italy: one, see, tour, great, place, take, around, go\n",
      "\n",
      "Tfidf Most Important Words:\n",
      "Venice, Italy: tour, see, one, great, place, take, grand, around\n"
     ]
    }
   ],
   "source": [
    "personalities = get_personality_words()\n",
    "non_personalities = []\n",
    "for words in tokenized_corpus_0:\n",
    "    for word in words:\n",
    "        syns = []\n",
    "        synonyms = wn.synsets(word.lower())\n",
    "        for s in synonyms:\n",
    "            synlist = [l.name() for l in s.lemmas()]\n",
    "            syns.extend(synlist)\n",
    "        if len(set.intersection(set(syns), personalities)) == 0:\n",
    "            non_personalities.append(word)\n",
    "            \n",
    "stop_wds = set(stopwords.words('english') + non_personalities)\n",
    "tokenized_corpus_1 = remove_stopwords(tokenized_corpus_0, stop_wds)\n",
    "print '{} stopwords removed'.format(len(stop_wds))\n",
    "print '\\nMost Common Words:'\n",
    "get_features_freq(['Venice, Italy'], cities, stop_wds, doc_bodies)\n",
    "print '\\nTfidf Most Important Words:'\n",
    "get_features_tfidf(['Venice, Italy'], cities, stop_wds, doc_bodies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk stop words and nouns removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nouns = limit_speech_part(tokenized_corpus_1, pts_of_speech = ['ADJ', 'ADJ_SAT', 'ADV', 'VERB'])\n",
    "stop_wds = set(stopwords.words('english') + non_personalities + nouns)\n",
    "tokenized_corpus_2 = remove_stopwords(tokenized_corpus_1, stop_wds)\n",
    "print '{} stopwords removed'.format(len(stop_wds))\n",
    "print '\\nMost Common Words:'\n",
    "get_features_freq(['Venice, Italy'], cities, stop_wds, doc_bodies)\n",
    "print '\\nTfidf Most Important Words:'\n",
    "get_features_tfidf(['Venice, Italy'], cities, stop_wds, doc_bodies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk stop words, nouns and verbs removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "verbs = limit_speech_part(tokenized_corpus_2, pts_of_speech = ['ADJ', 'ADJ_SAT', 'ADV'])\n",
    "stop_wds = set(stopwords.words('english') + non_personalities + nouns + verbs)\n",
    "tokenized_corpus_3 = remove_stopwords(tokenized_corpus_2, stop_wds)\n",
    "print '{} stopwords removed'.format(len(stop_wds))\n",
    "print '\\nMost Common Words:'\n",
    "get_features_freq(['Venice, Italy'], cities, stop_wds, doc_bodies)\n",
    "print '\\nTfidf Most Important Words:'\n",
    "get_features_tfidf(['Venice, Italy'], cities, stop_wds, doc_bodies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk stop words, nouns, verbs and adverbs removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adverbs = limit_speech_part(tokenized_corpus_3, pts_of_speech = ['ADJ', 'ADJ_SAT'])\n",
    "stop_wds = set(stopwords.words('english') + non_personalities + nouns + verbs + adverbs)\n",
    "tokenized_corpus_4 = remove_stopwords(tokenized_corpus_3, stop_wds)\n",
    "print '{} stopwords removed'.format(len(stop_wds))\n",
    "print '\\nMost Common Words:'\n",
    "get_features_freq(['Venice, Italy'], cities, stop_wds, doc_bodies)\n",
    "print '\\nTfidf Most Important Words:'\n",
    "get_features_tfidf(['Venice, Italy'], cities, stop_wds, doc_bodies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nltk stop words, nouns, verbs, adverbs, and common words removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "values, vocab, word_ids = plot_word_frequencies(tokenized_corpus_4, 'words removed')\n",
    "print 'Word in corpus: {}'.format(len(values))\n",
    "print 'Maximum word frequency: {}'.format(int(max(values)))\n",
    "vocab_dict = {i: word for i, word in enumerate(vocab)}\n",
    "# remove any words that are very common\n",
    "commonwords = word_ids[values > 3000]\n",
    "common_removelist = [vocab_dict[c] for c in commonwords]\n",
    "print common_removelist\n",
    "\n",
    "two_letter = [word for word in vocab if len(word) < 3]\n",
    "print two_letter\n",
    "\n",
    "stop_wds = set(stopwords.words('english') + non_personalities + nouns + verbs + adverbs + common_removelist + two_letter)\n",
    "tokenized_corpus_5 = remove_stopwords(tokenized_corpus_4, stop_wds)\n",
    "print '{} stopwords removed'.format(len(stop_wds))\n",
    "print '\\nMost Common Words:'\n",
    "get_features_freq(['Venice, Italy'], cities, stop_wds, doc_bodies)\n",
    "print '\\nTfidf Most Important Words:'\n",
    "get_features_tfidf(['Venice, Italy'], cities, stop_wds, doc_bodies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "commonwords = word_ids[values > 1500]\n",
    "common_removelist = [vocab_dict[c] for c in commonwords]\n",
    "print common_removelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a dictionary of personality words and their synonyms\n",
    "pers_dict = dict()\n",
    "for trait in personalities:\n",
    "    personality_words = []\n",
    "    synonyms = wn.synsets(trait.lower())\n",
    "    synlist = [word.lower()]\n",
    "    for s in synonyms:\n",
    "        synlist = [l.name() for l in s.lemmas()]\n",
    "        personality_words.extend([wordnet.lemmatize(w) for w in synlist if len(wn.synsets(w, pos=wn.ADJ))>0])\n",
    "    pers_dict[trait] = ' '.join(personality_words)\n",
    "\n",
    "# Convert dictionary into dataframe\n",
    "traits_df = pd.DataFrame.from_dict(pers_dict, orient='index', dtype=None)\n",
    "traits_df.columns = ['synonyms']\n",
    "\n",
    "# Extract cities and their descripitions from dataframe\n",
    "pers_doc_bodies = traits_df['synonyms'].values\n",
    "pers_tokenized_corpus = [word_tokenize(content.lower()) for content in pers_doc_bodies]\n",
    "traits = traits_df.index.values\n",
    "\n",
    "# Vectorize\n",
    "remove = ['mercenary', 'favourable', 'ill', 'high', 'ridden', 'hag', 'low', 'minded', 'well', 'self', 'pietistical',\n",
    "         'shoulder', 'meagre', 'zippy', 'long', 'prospicient', 'flown', 'janus', 'far', 'finished', 'goosy',\n",
    "         'anserine', 'pharisaical', 'outre', 'gonzo', 'pharisaic', 'pietistic', 'thou', 'ho', 'hum', 'silver',\n",
    "         'scotch', 'plaguy', 'faced', 'at_sea', 'double', 'crabbed', 'patient_of', 'first', 'two', 'laced',\n",
    "         'tongued', 'sizable', 'sizeable', 'slushy', 'sticking_out', 'wall', 'close', 'aerial', 'aery', 'aeriform',\n",
    "         'bribable', 'for_sale']\n",
    "vectorizer = TfidfVectorizer(stop_words=stopwords.words('english')+remove)\n",
    "X = vectorizer.fit_transform(pers_doc_bodies)\n",
    "features = vectorizer.get_feature_names()\n",
    "\n",
    "# Apply kmeans clustering and identify top features for each cluster\n",
    "kmeans = KMeans(n_clusters=12)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# 2. Find the top features for each cluster.\n",
    "top_centroids = kmeans.cluster_centers_.argsort()[:, -1:-8:-1]\n",
    "print \"top features for each cluster:\"\n",
    "for num, centroid in enumerate(top_centroids):\n",
    "    print \"%d: %s\" % (num, \", \".join(features[i] for i in centroid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pers_dict2 = dict()\n",
    "for trait in personalities:\n",
    "    personality_words = []\n",
    "    synonyms = wn.synsets(trait.lower())\n",
    "    synlist = [trait.lower()]\n",
    "    for s in synonyms:\n",
    "        synlist = [l.name() for l in s.lemmas()]\n",
    "        personality_words.extend([wordnet.lemmatize(w) for w in synlist if len(wn.synsets(w, pos=wn.ADJ))>0])\n",
    "    pers_dict2[trait] = personality_words\n",
    "\n",
    "syns = set([item for sublist in pers_dict2.values() for item in sublist])\n",
    "\n",
    "edges = []\n",
    "for syn in syns: \n",
    "    keys_lst = []\n",
    "    keys_lst = [key for key, value in pers_dict2.items() if syn in value]\n",
    "    if len(keys_lst) > 1:\n",
    "        edges.extend(list(itertools.combinations(keys_lst, 2)))\n",
    "        \n",
    "with open('personality_edges.txt', 'w') as f:\n",
    "    for edge in edges:\n",
    "        f.write(edge[0] + ' ' + edge[1] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot all of the nodes and edges in the graph\n",
    "fig = plt.figure(figsize = (7, 7))\n",
    "G = nx.read_edgelist('personality_edges.txt')\n",
    "pos = nx.spring_layout(G)\n",
    "# print len(list(nx.connected_components(G)))\n",
    "wordlist = ['cultured']\n",
    "for word in wordlist:\n",
    "    print G.neighbors(word)\n",
    "\n",
    "nx.draw(G, pos, node_size=20, node_color='m', edge_color='#191970')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# for l in list(nx.connected_components(G)):\n",
    "#     print l, '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pers_dict2 = dict()\n",
    "wordlist2 = '''peaceful, calm, tranquil, serene, quiet, harmonious, relaxed, easygoing\n",
    "carefree, comfortable, sedate, pastoral, laid_back, comfortable, easy'''.split(', ')\n",
    "\n",
    "for trait in wordlist2:\n",
    "    personality_words = [trait]\n",
    "    synonyms = wn.synsets(trait.lower())\n",
    "    synlist = [trait.lower()]\n",
    "    for s in synonyms:\n",
    "        synlist = [l.name() for l in s.lemmas()]\n",
    "        personality_words.extend([wordnet.lemmatize(w) for w in synlist])\n",
    "    pers_dict2[trait] = personality_words\n",
    "\n",
    "syns = set([item for sublist in pers_dict2.values() for item in sublist])\n",
    "\n",
    "edges = []\n",
    "for syn in syns: \n",
    "    keys_lst = []\n",
    "    keys_lst = [key for key, value in pers_dict2.items() if syn in value]\n",
    "    if len(keys_lst) > 1:\n",
    "        edges.extend(list(itertools.combinations(keys_lst, 2)))\n",
    "        \n",
    "with open('trait_edges.txt', 'w') as f:\n",
    "    for edge in edges:\n",
    "        f.write(edge[0] + ' ' + edge[1] + '\\n')\n",
    "\n",
    "fig = plt.figure(figsize = (5, 5))\n",
    "G = nx.read_edgelist('trait_edges.txt')\n",
    "pos = nx.spring_layout(G)\n",
    "\n",
    "nx.draw(G, pos, node_size=20, node_color='m', edge_color='#191970')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "synonyms = wn.synsets('relaxed'.lower())\n",
    "print synonyms\n",
    "synlist = []\n",
    "for s in synonyms:\n",
    "    synlist = [l.name() for l in s.lemmas()]\n",
    "    print synlist\n",
    "    personality_words.extend([wordnet.lemmatize(w) for w in synlist])\n",
    "print personality_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# personality_words = []\n",
    "# for line in f:\n",
    "#     for word in line.split():\n",
    "#         synonyms = wn.synsets(word.lower())\n",
    "#         for s in synonyms:\n",
    "#             synlist = [l.name() for l in s.lemmas()]\n",
    "#         synlist.append(word.lower())\n",
    "#         personality_words.extend([wordnet.lemmatize(w) for w in synlist if len(wn.synsets(w, pos=wn.ADJ))>0])\n",
    "#     return set(personality_words)\n",
    "\n",
    "synonyms = wn.synsets('peaceful')\n",
    "print synonyms\n",
    "for s in synonyms:\n",
    "    synlist = [l.name() for l in s.lemmas()]\n",
    "print synlist\n",
    "# w = [wordnet.lemmatize(w) for w in synlist if len(wn.synsets(w, pos=wn.ADJ))>0]\n",
    "w = [wordnet.lemmatize(w) for w in synlist]\n",
    "print w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot word frequency of remaining words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tokenized_corpus = tokenized_corpus_4\n",
    "# doc_bodies = docs_wordnet\n",
    "values, vocab, word_ids = plot_word_frequencies(tokenized_corpus_4, 'words removed')\n",
    "print 'Word in corpus: {}'.format(len(values))\n",
    "print 'Maximum word frequency: {}'.format(int(max(values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_dict = {i: word for i, word in enumerate(vocab)}\n",
    "\n",
    "# remove any words that are very common\n",
    "commonwords = word_ids[values > 4000]\n",
    "common_removelist = [vocab_dict[c] for c in commonwords]\n",
    "print common_removelist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stopwords and plot new frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_corpus_1 = remove_stopwords(tokenized_corpus)\n",
    "values, vocab, word_ids = plot_word_frequencies(tokenized_corpus_1, 'all words included')\n",
    "print 'Word in corpus: {}'.format(len(values))\n",
    "print 'Maximum word frequency: {}'.format(int(max(values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_dict = {i: word for i, word in enumerate(vocab)}\n",
    "\n",
    "# remove any words that are very common\n",
    "commonwords = word_ids[values > 2000]\n",
    "common_removelist = [vocab_dict[c] for c in commonwords]\n",
    "\n",
    "uncommonwords = word_ids[values == 1]\n",
    "uncommon_removelist = [vocab_dict[c] for c in uncommonwords]\n",
    "\n",
    "# remove two letter words|\n",
    "two_letter = [word for word in vocab if len(word) < 3]\n",
    "\n",
    "# Limit by part of speech\n",
    "remove_non = limit_speech_part(tokenized_corpus, ['ADJ', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "personalities = get_personality_words()\n",
    "non_personalities = []\n",
    "for words in tokenized_corpus:\n",
    "    for word in words:\n",
    "        syns = []\n",
    "        synonyms = wn.synsets(word.lower())\n",
    "        for s in synonyms:\n",
    "            synlist = [l.name() for l in s.lemmas()]\n",
    "            syns.extend(synlist)\n",
    "        if len(set.intersection(set(syns), personalities)) == 0:\n",
    "            non_personalities.append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "remove_more = ['turkish', 'german', 'english', 'russian', 'ukrainian', 'lithuanian', 'spanish', 'serbian',\n",
    "               'armenian', 'italian', 'greek', 'croatian', 'finnish', 'polish', 'estonian', 'french', 'swiss',\n",
    "               'czech', 'latvian', 'irish', 'danish', 'macedonian', 'welsh', 'bulgarian', 'british', 'norwegian',\n",
    "               'scottish', 'belgian', 'american', 'hungarian', 'cyrillic', 'european','dutch','gaelic','yugoslav',\n",
    "               'scandinavian','bosnia','herzegovina', 'europe', 'america', 'andorra', 'petersburg', 'catherine',\n",
    "               'romanov', 'peter', 'favoritethis', 'liechtenstein', 'austrian', 'flemish', 'georgian', 'tuscan', \n",
    "               'nazi', 'byzantine', 'roman', 'venetian', 'large', 'main', 'found', 'enough', 'located', 'done',\n",
    "               'second', 'full', 'booked', 'right', 'square', 'rum', 'ordered', 'easy', 'spent', 'sure', 'august',\n",
    "              'near', 'fly', 'still' ,'flip', 'becoming', 'minute', 'laid', 'arch', 'set', 'long', 'black', 'animal',\n",
    "              'longer', 'used','able', 'whole', 'last', 'big', 'high', 'star', 'come', 'close', 'bit', 'line' ,'new',\n",
    "              'train', 'think', 'left', 'end', 'say', 'child', 'pas', 'drive', 'white', 'offer', 'guided', 'experience',\n",
    "              'space', 'know', 'cannot', 'situated', 'closed', 'thorough', 'fantastic', 'excellent', 'better','special',\n",
    "               'perfect','taking','short','house', 'base', 'golden', 'blue', 'gold', 'driving','inner','kart','dress','pronounced',\n",
    "              'loved','ms','perpendicular','open','game','past','experienced','contained','home', 'weather', 'age', \n",
    "               'wanted', 'start','dog', 'ground','el','variety', 'range', 'hard', 'check', 'spot', 'sculpture','floor','point',\n",
    "              'spot', 'give', 'thought','quarter','seat', 'lower','behind','however','late','nearly','fairly','live',\n",
    "              'interested']\n",
    "\n",
    "\n",
    "# remove any words that are very common\n",
    "common = word_ids[values > 2000]\n",
    "common_list = [vocab_dict[c] for c in common]\n",
    "tokenized_corpus = remove_stopwords(tokenized_corpus, common_removelist + two_letter + uncommon_removelist + \n",
    "                                    remove_more + non_personalities + common_list)\n",
    "\n",
    "# tokenized_corpus = remove_stopwords(tokenized_corpus)\n",
    "\n",
    "# plot the word frequencies\n",
    "values, vocab, word_ids = plot_word_frequencies(tokenized_corpus, 'custom stop words removed')\n",
    "\n",
    "vocab_dict = {i: word for i, word in enumerate(vocab)}\n",
    "\n",
    "\n",
    "print common_list\n",
    "print 'Word in corpus: {}'.format(len(values))\n",
    "print 'Maximum word frequency: {}'.format(int(max(values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stops = stopwords.words('english')\n",
    "vectorizer = TfidfVectorizer(stop_words=stops + remove_more + remove_non + common_removelist + common_list + non_personalities)\n",
    "# vectorizer = CountVectorizer(stop_words=stops + remove_more + uncommon_removelist + common_list +\n",
    "#                              common_removelist + non_personalities)\n",
    "X = vectorizer.fit_transform(doc_bodies)\n",
    "features = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_dense = X.todense()\n",
    "# print x_dense\n",
    "# print x_dense.max()\n",
    "for i, doc in enumerate(x_dense):\n",
    "    d = np.array(doc)[0]\n",
    "    print cities[i]\n",
    "    top_indices = np.array(np.argsort(d)[::-1][:8])\n",
    "    print np.array(features)[top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=8)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# 2. Find the top features for each cluster.\n",
    "top_centroids = kmeans.cluster_centers_.argsort()[:, -1:-13:-1]\n",
    "print \"top features for each cluster:\"\n",
    "for num, centroid in enumerate(top_centroids):\n",
    "    print \"%d: %s\" % (num, \", \".join(features[i] for i in centroid))\n",
    "\n",
    "# 3. Print out the titles of a random sample of the articles assigned to each\n",
    "# cluster to get a sense of the topic.\n",
    "# assigned_cluster = kmeans.transform(X).argmin(axis=1)\n",
    "# for i in range(kmeans.n_clusters):\n",
    "#     cluster = np.arange(0, X.shape[0])[assigned_cluster == i]\n",
    "#     sample_cities = np.random.choice(cluster, 2, replace=False)\n",
    "#     print \"cluster %d:\" % i\n",
    "#     for city in sample_cities:\n",
    "#         print \"    %s\" % cities[city].strip('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print x_dense.shape\n",
    "\n",
    "# indices = np.argsort(vectorizer.idf_)[::-1]\n",
    "# features = vectorizer.get_feature_names()\n",
    "# top_n = 2\n",
    "# top_features = [features[i] for i in indices[:top_n]]\n",
    "# print top_features\n",
    "# print vectorizer.idf_.shape\n",
    "\n",
    "# features_dict = defaultdict(list)\n",
    "# for f, w in zip(vectorizer.get_feature_names(), vectorizer.idf_):\n",
    "#     features_dict[len(f.split(' '))].append((f, w))\n",
    "# top_n = 2\n",
    "# for gram, features in features_dict.iteritems():\n",
    "#     top_features = sorted(features, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "#     top_features = [f[0] for f in top_features]\n",
    "#     print '{}-gram top:'.format(gram), top_features\n",
    "from itertools import izip\n",
    "\n",
    "def sort_coo(m):\n",
    "    tuples = izip(m.row, m.col, m.data)\n",
    "    return sorted(tuples, key=lambda x: (x[0], x[2]))\n",
    "\n",
    "for entry in X.shape[0:\n",
    "    d = X.getrow(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement K-means Clustering to group cities by common features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove_words = []\n",
    "# # options are ADJ, ADV, VERB, NOUN\n",
    "# for content in doc_bodies:\n",
    "#     for word in content.split():\n",
    "#         speech_check = wn.synsets(word, pos=wn.ADJ)\n",
    "#         if len(speech_check) == 0:\n",
    "#             remove_words.append(word)\n",
    "# #         else:\n",
    "# #             print word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means Clustering Limiting by Part of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordnet = WordNetLemmatizer()\n",
    "# read list of personality words\n",
    "with open('personalitywords.txt') as f:\n",
    "    personality_words = []\n",
    "    for line in f:\n",
    "        for word in line.split():\n",
    "            synonyms = wn.synsets(word.lower())\n",
    "            for s in synonyms:\n",
    "                synlist = [l.name() for l in s.lemmas()]\n",
    "            synlist.append(word.lower())\n",
    "            personality_words.extend([wordnet.lemmatize(w) for w in synlist])\n",
    "            \n",
    "# make dictionaries out of rick steves data\n",
    "rs_dict = json_to_dict('ricksteves3.json')\n",
    "\n",
    "\n",
    "\n",
    "# Convert dictionary into dataframe\n",
    "cities_df = pd.DataFrame.from_dict(rs_dict, orient='index', dtype=None)\n",
    "cities_df.columns = ['description']\n",
    "\n",
    "# Extract cities and their descripitions from dataframe\n",
    "doc_bodies = cities_df['description'].values\n",
    "cities = cities_df.index.values\n",
    "\n",
    "# Create a tokenized version of the corpus for frequency plots\n",
    "tokenized_corpus = [word_tokenize(content.lower()) for content in doc_bodies]\n",
    "# lemmatize\n",
    "\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "\n",
    "docs_wordnet = [' '.join([wordnet.lemmatize(word) for word in words.split()]) for words in doc_bodies]\n",
    "# docs_snowball = [' '.join([snowball.stem(word) for word in words.split()]) for words in doc_bodies]\n",
    "# docs_porter = [' '.join([porter.stem(word) for word in words.split()]) for words in doc_bodies]\n",
    "\n",
    "tokenized_corpus_wn = [word_tokenize(content.lower()) for content in docs_wordnet]\n",
    "# tokenized_corpus_p = [word_tokenize(content.lower()) for content in docs_porter]\n",
    "# tokenized_corpus_s = [word_tokenize(content.lower()) for content in docs_snowball]\n",
    "\n",
    "# for wl, w in zip(tokenized_corpus[1], tokenized_corpus_wn[1]):\n",
    "#     if wl != w:\n",
    "#         print wl, w\n",
    "        \n",
    "tokenized_corpus = tokenized_corpus_wn\n",
    "doc_bodies = docs_wordnet\n",
    "\n",
    "# tokenized_corpus = tokenized_corpus_s\n",
    "# doc_bodies = docs_snowball\n",
    "\n",
    "# tokenized_corpus = tokenized_corpus_p\n",
    "# doc_bodies = docs_porter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove nltk stopwords\n",
    "tokenized_corpus = remove_stopwords(tokenized_corpus)\n",
    "\n",
    "# plot the word frequencies\n",
    "values, vocab, word_ids = plot_word_frequencies(tokenized_corpus, 'nltk stopwords removed')\n",
    "\n",
    "print 'Word in corpus: {}'.format(len(values))\n",
    "print 'Maximum word frequency: {}'.format(int(max(values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_dict = {i: word for i, word in enumerate(vocab)}\n",
    "\n",
    "# remove any words that appears more than 200 times\n",
    "commonwords = word_ids[values > 200]\n",
    "common_removelist = [vocab_dict[c] for c in commonwords]\n",
    "print common_removelist\n",
    "\n",
    "# remove words that are not personality words\n",
    "non_pers_words = [word for word in vocab if word not in set(personality_words)]\n",
    "tokenized_corpus = remove_stopwords(tokenized_corpus, common_removelist)\n",
    "\n",
    "# plot the word frequencies\n",
    "values, vocab, word_ids = plot_word_frequencies(tokenized_corpus, 'common words removed')\n",
    "\n",
    "print 'Words in corpus: {}'.format(len(values))\n",
    "print 'Maximum word frequency: {}'.format(int(max(values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "remove_more = ['cathedral', 'center', 'house', 'home', 'der', 'turkish', 'german', 'english', 'russian', 'ukrainian',\n",
    "              'lithuanian', 'spanish', 'serbian', 'armenian', 'italian', 'greek', 'croatian', 'finnish', 'polish',\n",
    "              'estonian', 'like', 'french', 'swiss', 'czech', 'next', 'northwest', 'southwest', 'latvian', 'irish',\n",
    "              'side', 'danish', 'macedonian', 'welsh', 'bulgarian', 'british', 'august', 'pass', 'overnight', 'rick',\n",
    "              'norwegian', 'scottish', 'belgian', 'american', 'mid', 'second', 'hungarian', 'domestic', 'zero','terminal',\n",
    "              'within', 'one','cyrillic','select','key','first','much','back','still','made','every','north','several',\n",
    "              'world','long','extra','firsthand','european','five','dutch','top','done','gaelic','yugoslav','three','south',\n",
    "              'scandinavian','bosnia','herzegovina','dime','nickel','cannot','right','left','minute','taking','low',\n",
    "              'behind']\n",
    "\n",
    "stops = stopwords.words('english')\n",
    "vectorizer = TfidfVectorizer(stop_words=stops+remove_words+remove_more)\n",
    "X = vectorizer.fit_transform(doc_bodies)\n",
    "features = vectorizer.get_feature_names()\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# 2. Find the top features for each cluster.\n",
    "top_centroids = kmeans.cluster_centers_.argsort()[:, -1:-13:-1]\n",
    "print \"top features for each cluster:\"\n",
    "for num, centroid in enumerate(top_centroids):\n",
    "    print \"%d: %s\" % (num, \", \".join(features[i] for i in centroid))\n",
    "\n",
    "# 3. Print out the titles of a random sample of the articles assigned to each\n",
    "# cluster to get a sense of the topic.\n",
    "assigned_cluster = kmeans.transform(X).argmin(axis=1)\n",
    "for i in range(kmeans.n_clusters):\n",
    "    cluster = np.arange(0, X.shape[0])[assigned_cluster == i]\n",
    "    sample_cities = np.random.choice(cluster, 2, replace=False)\n",
    "    print \"cluster %d:\" % i\n",
    "    for city in sample_cities:\n",
    "        print \"    %s\" % cities[city].strip('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matplotlib.use('Agg')\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "m = Basemap(projection='stere', lon_0=5, lat_0=90.0, rsphere=6371200., llcrnrlon=-25.0,\n",
    "            urcrnrlon=90.0, llcrnrlat=26.0, urcrnrlat=50.0, resolution='l')\n",
    "colors = ['r', 'g', 'b', 'm', 'c', 'k', 'y']\n",
    "geolocator = Nominatim()\n",
    "for i in range(kmeans.n_clusters):\n",
    "    cluster = np.arange(0, X.shape[0])[assigned_cluster == i]\n",
    "    for city in cluster:\n",
    "        location = geolocator.geocode(cities[city], timeout=10)\n",
    "        if location is not None:\n",
    "            xpt, ypt = m(location.longitude, location.latitude)\n",
    "            plt.plot(xpt, ypt, '.', color=colors[i], alpha=0.5, markersize=20)\n",
    "\n",
    "m.drawcoastlines(linewidth=0.2)\n",
    "m.drawcountries(linewidth=0.2)\n",
    "plt.savefig('city_clusters.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ambiance = ['relaxing','calm','unwind','laze','soothing','harmonious','mild','serene','slow','tranquil',\n",
    "           'quiet','peaceful','soft','lounging']\n",
    "syns = build_synonym_list(ambiance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cosine Similarity using TF-IDF\n",
    "# type(doc_bodies)\n",
    "# type(cities)\n",
    "\n",
    "doc_bodies.append(' '.join(syns))\n",
    "stops = stopwords.words('english')\n",
    "vectorizer = TfidfVectorizer(stop_words= stops)\n",
    "\n",
    "db = doc_bodies\n",
    "X = vectorizer.fit_transform(db)\n",
    "# print len(doc_bodies)\n",
    "# 1. Compute cosine similarity\n",
    "cosine_similarities = linear_kernel(X, X)\n",
    "\n",
    "sims = np.zeros([len(db),])\n",
    "# 2. Print out similarities\n",
    "for i, doc1 in enumerate(db):\n",
    "    for j, doc2 in enumerate(db):\n",
    "        if i == len(db) - 1:\n",
    "            sims[j] = cosine_similarities[i, j]\n",
    "#             print i, j, cosine_similarities[i, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print cities[np.argmax(sims[:-1])]\n",
    "max(sims[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print doc_bodies['Hamburg, Germany']\n",
    "\n",
    "for word in doc_bodies[np.argmax(sims)].split():\n",
    "    if word in set(syns):\n",
    "        print word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_bodies[np.argmax(sims)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "us_dict = json_to_dict('us_cities.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('us_cities.txt', 'w') as f:\n",
    "    for key in us_dict:\n",
    "        f.write(key + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "m = Basemap(projection='stere', lon_0=5, lat_0=90.0, rsphere=6371200., llcrnrlon=-25.0,\n",
    "            urcrnrlon=90.0, llcrnrlat=26.0, urcrnrlat=50.0, resolution='l')\n",
    "colors = ['r', 'g', 'b', 'm', 'c', 'k', 'y']\n",
    "geolocator = Nominatim()\n",
    "for city in cities:\n",
    "    location = geolocator.geocode(city, timeout=10)\n",
    "    if location is not None:\n",
    "        xpt, ypt = m(location.longitude, location.latitude)\n",
    "        plt.plot(xpt, ypt, '.', color='g', markersize=10)\n",
    "        \n",
    "citlist = ['Venice, Italy', 'Paris, France', 'Rome, Italy', 'London, England', 'Moscow, Russia', 'Athens, Greece',\n",
    "          'Bern, Switzerland']\n",
    "for c in citlist:\n",
    "    location = geolocator.geocode(c, timeout=10)\n",
    "    xpt, ypt = m(location.longitude, location.latitude)\n",
    "    plt.plot(xpt, ypt, '.', color='m', markersize=10)\n",
    "\n",
    "m.drawcoastlines(linewidth=0.2)\n",
    "m.drawcountries(linewidth=0.2)\n",
    "plt.savefig('city_example.png', bbox_inches='tight')\n",
    "m.drawmapboundary(fill_color='#1E90FF')\n",
    "# fill continents, set lake color same as ocean color.\n",
    "m.fillcontinents(color='#F5DEB3',lake_color='#1E90FF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
